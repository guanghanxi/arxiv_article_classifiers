{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef84b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9e51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917bde8",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39665a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"selected_articles.json\", 'r') as file:\n",
    "    selected_articles = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame(selected_articles)[[\"id\", \"title\", \"authors\", \"categories\", \"abstract\"]]\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = [\"cs.AI\", \"cs.DB\", \"cs.IT\", \"cs.LG\", \"cs.SI\"]\n",
    "category_dict = {\"cs.AI\":0, \"cs.DB\":1, \"cs.IT\":2, \"cs.LG\":3, \"cs.SI\":4}\n",
    "stopwords = ['are', 'was', 'were', 'been', 'being', 'did', 'done', 'had', 'has', 'have', 'will', 'would', 'may', 'might', 'should', 'can', 'could', \n",
    "'she', 'her', 'you', 'your', 'him', 'they', 'them', 'ours', 'its', 'ourselves', 'yourself', 'itself', 'himself', 'yourselves', 'herself', 'himself',\n",
    "'that', 'there', 'here', 'this', 'the', 'these', 'those', 'other', 'for', 'with', 'through', 'off', 'from', 'about','under', 'between', 'below', 'above', 'out', 'than',\n",
    "'yes', 'not', 'and', 'because', 'nor', 'neither', 'then', 'but', 'too', 'else', 'also', 'either', 'when', 'what', 'which', 'who', 'why', 'whose', 'where', 'how',\n",
    "'when', 'while', 'all', 'both', 'only', 'every', 'ever', 'much', 'more', 'many', 'very']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc38d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_tensor(id_matrix):\n",
    "    sparse_mx = id_matrix.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse_coo_tensor(indices, values, shape)\n",
    "\n",
    "def built_vocabulary(traindata, tokenizer):\n",
    "    vocabulary = []\n",
    "    traindata['word_list'] = traindata['abstract'].apply(lambda x: [word.lower() for word in tokenizer.tokenize(x)])\n",
    "    for i, v in tqdm(traindata['word_list'].iteritems()):\n",
    "        vocabulary += v\n",
    "    voc_dict = Counter(vocabulary)\n",
    "    uniq_voc = list(set(vocabulary))\n",
    "    for word in tqdm(uniq_voc):\n",
    "        if (voc_dict[word]<5) or (word in stopwords):\n",
    "            voc_dict.pop(word)\n",
    "    word_place_dict={}\n",
    "    uniq_voc = list(voc_dict.keys())\n",
    "    for i in tqdm(range(len(uniq_voc))):\n",
    "        word_place_dict[uniq_voc[i]] = i\n",
    "    return word_place_dict\n",
    "    \n",
    "# Build the term-document matrix, and generate ground truth array\n",
    "def process_data(data_df, word_dict, categories, category_id,\n",
    "                                if_train = True, tokenizer = None): \n",
    "    uniq_words = word_dict.keys()\n",
    "    ground_truth = np.zeros((len(data_df), len(categories)))\n",
    "    td_matrix = sparse.lil_matrix((data_df.shape[0], len(uniq_words))) # The term-document matrix\n",
    "    \n",
    "    # For evaluation and test, word_list need \n",
    "    if not if_train:\n",
    "        data_df['word_list'] = data_df['abstract'].apply(lambda x: [word.lower() for word in tokenizer.tokenize(x)])\n",
    "    \n",
    "    for idx, row in tqdm(data_df.iterrows()):\n",
    "        tmp = Counter(row['word_list'])\n",
    "        for k,v in tmp.items():\n",
    "            if k in uniq_words:\n",
    "                td_matrix[idx, word_dict[k]] = v\n",
    "        \n",
    "        c_list = row['categories'].split(\" \")\n",
    "        for c in c_list:\n",
    "            if c in categories:\n",
    "                ground_truth[idx][category_id[c]] = 1  \n",
    "                \n",
    "    bias_td_matrix = torch.hstack((to_sparse_tensor(td_matrix), torch.ones(len(data_df),1).to_sparse()))        \n",
    "    return bias_td_matrix, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86073b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the train set, evaluate set and test set\n",
    "train_df = total_df.iloc[:180000]\n",
    "valid_df = total_df.iloc[180000:192000]\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = total_df.iloc[192000:]\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "word_dict = built_vocabulary(train_df, regtokenizer)\n",
    "train_X, train_Y_np = process_data(train_df, word_dict, category_list, category_dict)\n",
    "valid_X, valid_Y_np = process_data(valid_df, word_dict, category_list, category_dict, False, regtokenizer)\n",
    "test_X, test_Y_np = process_data(test_df, word_dict, category_list, category_dict, False, regtokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33129bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(word_dict.keys())+1\n",
    "count = len(total_df)\n",
    "\n",
    "train_Y = torch.from_numpy(train_Y_np).float()\n",
    "valid_Y = torch.from_numpy(valid_Y_np).float()\n",
    "test_Y = torch.from_numpy(test_Y_np).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b6505",
   "metadata": {},
   "source": [
    "## Define Evaluate and Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92248c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(devX, devY, Model, device, loss_fn, batch_size = 16, iftest = False):\n",
    "    Model.eval()\n",
    "    dev_count = devY.shape[0]\n",
    "    dev_num_batch = dev_count//batch_size\n",
    "    indices = torch.arange(batch_size)\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    for i in tqdm(range(dev_num_batch), desc = \"Evaluation step\", leave = False):\n",
    "        outputs = Model(devX.index_select(0, indices).to(device))\n",
    "        ground_truth = devY[indices].to(device)\n",
    "        total_acc += (((outputs>0.5).float() == ground_truth).float()).mean()\n",
    "        loss = loss_fn(outputs, ground_truth)\n",
    "        total_loss += loss.item()\n",
    "        indices = indices + batch_size\n",
    "        \n",
    "    return total_acc/dev_num_batch, total_loss/dev_num_batch        \n",
    "\n",
    "def test(devX, devY, Model, device, batch_size = 16):\n",
    "    Model.eval()\n",
    "    dev_count = devY.shape[0]\n",
    "    cag_num = devY.shape[1]\n",
    "    dev_num_batch = dev_count//batch_size\n",
    "    indices = torch.arange(batch_size)\n",
    "    total_acc = 0\n",
    "    tp_fp_fn = np.ones((3, cag_num))\n",
    "    for i in tqdm(range(dev_num_batch), desc = \"Evaluation step\", leave = False):\n",
    "        outputs = Model(devX.index_select(0, indices).to(device))\n",
    "        ground_truth = devY[indices].to(device)\n",
    "        classify_results = (outputs>0.5).float()\n",
    "        total_acc += ((classify_results == ground_truth).float()).mean()\n",
    "        for j in range(batch_size):\n",
    "            for k in range(cag_num):\n",
    "                if classify_results[j][k]==1:\n",
    "                    if classify_results[j][k] == ground_truth[j][k]:\n",
    "                        tp_fp_fn[0][k] += 1\n",
    "                    else:\n",
    "                        tp_fp_fn[1][k] += 1\n",
    "                        \n",
    "                else:\n",
    "                    if ground_truth[j][k]==1:\n",
    "                        tp_fp_fn[2][k] += 1\n",
    "                \n",
    "        indices = indices + batch_size\n",
    "        \n",
    "    precision = tp_fp_fn[0]/(tp_fp_fn[0]+tp_fp_fn[1])\n",
    "    recall = tp_fp_fn[0]/(tp_fp_fn[0]+tp_fp_fn[2])\n",
    "    macrof1 = 2/(1/precision.mean() + 1/recall.mean())\n",
    "    mean_tp_fp_fn = tp_fp_fn.mean(axis = 1)\n",
    "    microf1 = 2*mean_tp_fp_fn[0]/(2*mean_tp_fp_fn[0] + mean_tp_fp_fn[1] + 2*mean_tp_fp_fn[2])\n",
    "    print(tp_fp_fn)\n",
    "        \n",
    "    return total_acc/dev_num_batch, macrof1, microf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617eb18",
   "metadata": {},
   "source": [
    "## Define the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, n, num_class):\n",
    "        super(DNN, self).__init__()\n",
    "        self.first_layer = nn.Linear(n, 256)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.second_layer = nn.Linear(256, 5)\n",
    "        self.output_layer = nn.Sigmoid()\n",
    "        \n",
    "        # Initialize\n",
    "        self.first_layer.weight.data.uniform_(-1e-5, 1e-5)\n",
    "        self.second_layer.weight.data.uniform_(-1e-5, 1e-5)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        return self.output_layer(self.second_layer(self.activation(self.first_layer(feature))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b491d",
   "metadata": {},
   "source": [
    "## Define Parameters and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05594f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = DNN(dim, 5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "train_count = len(train_df)\n",
    "loss_record = [] \n",
    "eval_accuracy_list = []\n",
    "eval_loss_list = []\n",
    "k = 0\n",
    "num_batch = train_count//batch_size\n",
    "tr_loss = 0\n",
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs), desc = \"epoch\", leave = True):\n",
    "    indices = torch.arange(batch_size)\n",
    "    for i in tqdm(range(num_batch), desc = \"step\", leave = False):\n",
    "        outputs = model.forward(train_X.index_select(0, indices).to(device))\n",
    "        loss = loss_function(outputs, train_Y[indices].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        k += 1\n",
    "        tr_loss += loss.item()\n",
    "        indices = indices + batch_size\n",
    "        if k%100 == 0 and k!=0:\n",
    "            loss_record.append(tr_loss/100)            \n",
    "            if k%1000 == 0:\n",
    "                eval_acc, eval_loss = evaluate(valid_X, valid_Y, model, device, loss_function, batch_size)\n",
    "                eval_accuracy_list.append(eval_acc)\n",
    "                eval_loss_list.append(eval_loss)\n",
    "                print(\"Mean Train Loss\", tr_loss/100, \"Mean evaluation accuracy\", eval_acc, \n",
    "                     \"Mean evaluation loss\", eval_loss)\n",
    "                model.train()\n",
    "                \n",
    "            tr_loss = 0\n",
    "     \n",
    "    # save model after each epoch\n",
    "    save_path = \"./MLP_checkpoint/checkpoint-\"+str(epoch+1)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    torch.save(model, save_path + \"/mlp_model_\"+str(epoch+1)+\".pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9a6b18",
   "metadata": {},
   "source": [
    "## Save Results and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.array([loss_record, np.arange(100, num_epochs*num_batch+1, 100)])\n",
    "train_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90810aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and evaluation results for the final\n",
    "end_eval_acc, end_eval_loss = evaluate(valid_X, valid_Y, model, device, loss_function, batch_size)\n",
    "eval_accuracy_list.append(end_eval_acc)\n",
    "eval_loss_list.append(end_eval_loss)\n",
    "end_eval_acc, end_eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8da567",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_steps = np.arange(1000, num_epochs*num_batch+1, 1000)\n",
    "eval_steps = np.append(eval_steps, num_epochs*num_batch)\n",
    "eval_results = np.array([eval_accuracy_list, eval_loss_list, eval_steps])\n",
    "eval_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "np.savez(\"MLP_results.npz\", train_loss, eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c45379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test(test_X, test_Y, model, device, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
